{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LDA_blei_implement.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "S14RCJb5OOAl"
      ],
      "toc_visible": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.1"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "0n4sOQ2CON-v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Topic Modelling with Latent Dirichlet Allocation\n",
        "==\n",
        "Paper: Latent Dirichlet Allocation\n",
        "\n",
        "Authors: David M Blei, Andrew Y Ng, Michael I Jordan "
      ]
    },
    {
      "metadata": {
        "id": "-q3XllHqON-0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1. Introduction"
      ]
    },
    {
      "metadata": {
        "id": "yIzYg1k2ON-3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In natural language processing, **Latent Dirichlet Allocation (LDA)** is a widely used topic model proposed by David Blei, Andrew Ng, and Michael Jordan, capable of automatically discovering topics that documents in a corpus contain and explaining similarities between documents. LDA is a three-level hierarchical Bayesian model, and topic modeling is a classic problem in natural language processing. \n",
        "\n",
        "In the following report, we first describe the mechanism of Latent Dirichlet Allocation. We then use two methods to implement LDA: Variational Inference and Collapsed Gibbs Sampling."
      ]
    },
    {
      "metadata": {
        "id": "Tw2uKiSDON-6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2. Algorithm Description and Notations"
      ]
    },
    {
      "metadata": {
        "id": "YCcUnl_DON-_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "LDA uses a generative model to explain how the observed words in the documents of a corpus are generated from latent variables. The following shows the graphical model representation of LDA:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "95fd670f-4b3b-4bae-f553-83d063e2d50a",
        "id": "-OchM-k0hLR4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        }
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image('LDA.png')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAikAAAEACAIAAAD5unEqAAAAAXNSR0IArs4c6QAAAARnQU1BAACx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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "_OE6TFVHON_F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The boxes are \"plates\" representing replicates. The outer plate represents documents, while the inner plate represents the repeated choice of topics and words within a document. M denotes the number of document, N the number of words in a documents, and V indicates the size of the vocabulary of the corpus. We define the following terms:\n",
        "\n",
        "* $\\alpha$ is the parameter of the Dirichlet prior on the per-document topic distribution, \n",
        "* $\\beta_i$ is the word distribution for topic k\n",
        "* $\\theta_m$ is the topic distribution for document m,\n",
        "* $z_{mw}$ is the topic of word w in document m\n",
        "\n",
        "LDA assumes the following generative process for each document **m** in a corpus:\n",
        "1. Choose $N$ ~ Poisson($\\xi$).\n",
        "2. Choose $\\theta$ ~ Dir($\\alpha$).\n",
        "3. For each of the $N$ word $w_n$:\n",
        "    1. Choose a topic $z_n$ ~ Multinomial($\\theta$).\n",
        "    2. Choose a word $w_n$ from $p(w_n |z_n, \\beta)$, a multinomial probability conditioned on the topic $z_n$.\n",
        "    \n",
        "Note that the length of each document $N$ does not interact with any of the other variables and is thus just assumed to be known."
      ]
    },
    {
      "metadata": {
        "id": "6iVwzTcCON_H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### The Dirichlet Distribution"
      ]
    },
    {
      "metadata": {
        "id": "C4KA7bm8ON_K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The **Dirichlet Distribution** is the multivariate generalization of the beta distribution, which means the Dirichlet distribution is a distribution over discrete probability distributions. Dirichlet distributions are oftenly used as conjugate prior distributions of the categorical distribution and multinomial distribution in Bayesian statistics. \n",
        "\n",
        "A *k*-dimensional Dirichlet random variable $\\theta$ can take values in the (k-1)-simplex (a k-vector $\\theta$ lies in the (k-1)-simplex if ${ \\theta  }_{ i }\\ge 0,\\sum _{ i }^{ k }{ { \\alpha  }_{ i } } $), and has the following probability density on the simplex:\n",
        "\n",
        "$$p(\\theta| \\alpha) = \\frac { \\Gamma (\\sum _{ i=1 }^{ k }{ { \\alpha  }_{ i } } ) }{ \\prod _{ i=1 }^{ k }{ \\Gamma ({ \\alpha  }_{ i }) }  } { { \\theta  }_{ 1 } }^{ { \\alpha  }_{ 1 }-1 }\\cdot \\cdot \\cdot { { \\theta  }_{ k } }^{ { \\alpha  }_{ k }-1 }$$"
      ]
    },
    {
      "metadata": {
        "id": "VHqYiNSuON_N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Given the parameters $\\alpha$ and $\\beta$, the joint distribution of a topic mixture $\\theta$, a set of $N$ topics $z$, and a set of $N$ words $w$ is given by:\n",
        "\n",
        "$$\n",
        "p(\\theta, z, w|\\alpha, \\beta)=p(\\theta|\\alpha)\\prod _{ n=1 }^{ N }{ p(z_n|\\theta)p(w_n|z_n,\\beta) } \n",
        "$$\n",
        "\n",
        "where $p(z_n|\\theta)$ is simply $\\theta_i$ for the unique i such that ${ z }_{ n }^{ i }=1$. Integrating over $\\theta$ and summing over z, we obtain the marginal distribution of a document:\n",
        "\n",
        "$$\n",
        "p(w|\\alpha, \\beta) = \\int { p(\\theta |\\alpha )(\\prod _{ n=1 }^{ N }{ \\sum { p({ z }_{ n }|\\theta )p({ w }_{ n }|{ z }_{ n },\\beta ) }  } )d\\theta  } \n",
        "$$\n",
        "\n",
        "Finally, taking the product of the marginal probabilities of single documents, we obtain the probability of a corpus:\n",
        "\n",
        "$$\n",
        "p(D|\\alpha, \\beta) = \\prod _{ d=1 }^{ M }{ \\int { p(\\theta_d |\\alpha )(\\prod _{ n=1 }^{ N_d }{ \\sum { p({ z }_{ dn }|\\theta )p({ w }_{ dn }|{ z }_{ dn },\\beta ) }  } )d\\theta_d  }  } \n",
        "$$"
      ]
    },
    {
      "metadata": {
        "id": "omR0rTK1ON_S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To infer the latent parameters is a problem of Bayesian inference. Next, we use Variational Inference and Gibbs Sampling to estimate latent parameters."
      ]
    },
    {
      "metadata": {
        "id": "5-xau3A1ON_V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.1 Implementation - Gibbs Sampling"
      ]
    },
    {
      "metadata": {
        "id": "aVJil1EoON_W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the original paper, Blei, Ng and Jordan (2002) gave a variational inference approximation of the posterior distribution, because posterior distribution is usually intractable. Since then though, Gibbs Sampling has also become a commonly used way to infer latent parameters in LDA. Here, we use Gibbs Sampling to implement LDA. Gibbs Sampling is a Markov Chain Monte Carlo method, in which the next state is reached by sequentially sampling from the full conditional distributions of all other variables and the data."
      ]
    },
    {
      "metadata": {
        "id": "KXf-hRZ5ON_Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Since the Dirichlet distribution is conjugate prior of the multinomial distribution, the posteriors of $\\theta_i$ and $\\beta_i$ also follow the Dirichlet distribution. Their posterior means are:\n",
        "\n",
        "$$\n",
        "\\theta_{i,k} = \\frac { { n }_{ i }^{ k }+{ \\alpha  }_{ k } }{ \\sum _{ k=1 }^{ K }{ { n }_{ i }^{ k }+{ \\alpha  }_{ k } }  } \n",
        "$$\n",
        "\n",
        "$$\n",
        "\\beta_{k,w} = \\frac { { n }_{ w }^{ k }+{ \\beta  }_{ w } }{ \\sum _{ w=1 }^{ W }{ { n }_{ w }^{ k }+{ \\beta  }_{ w } }  } \n",
        "$$\n",
        "\n",
        "where ${ n }_{ i }^{ k }$ is the number of words in document i that have been assigned to topic k, ${ n }_{ w }^{ k }$ is the total number words $w$ assigned to topic $k$ among all documents in the corpus.\n",
        "\n",
        "Obviously, the inference of $\\theta$ and $\\beta$ only depends on assignments of each word to topics $z_i$. Therefore, we can only focus on estimation of $z_i$. We define some terms:\n",
        "\n",
        "* $n_m$: the word count of document $m$, not including the current one \n",
        "* $n_{mz}$: the number of words from document $m$ assigned to topic $z$, not including the current one\n",
        "* $n_{zw}$: the number of instances of word $w$ assigned to topic $z$, not including the current one\n",
        "* $n_z$: the total number of words assigned to topic $z$, not including the current one"
      ]
    },
    {
      "metadata": {
        "id": "RY_Bk7RbON_Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Then, the posterior distribution of word assignment is:\n",
        "\n",
        "$$\n",
        "p(z_i=j|z_i,w)\\propto \\frac { n_{zw} + \\phi }{ n_z + V\\phi } \\cdot \\frac { n_{mz} + \\alpha }{ n_m + K\\alpha } \n",
        "$$\n",
        "\n",
        "And we can implement LDA by Gibbs Sampling."
      ]
    },
    {
      "metadata": {
        "id": "ycThFJgION_a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Parameters\n",
        "\n",
        "document:    $m = 1,...,M$\n",
        "\n",
        "topic asigned to word:       $z = 1,...,K$\n",
        "\n",
        "word:        $w = 1,...,N_V$\n",
        "\n",
        "vocabulary : $v = 1,...,V$\n",
        "\n",
        "Z: topic assigned to word w\n",
        "\n",
        "$\\theta: K \\times N$ \n",
        "\n",
        "$\\beta: M \\times K$ \n",
        "\n",
        "$Multinomial(\\theta)$: distribution over words for a given topic\n",
        "\n",
        "$Multinomial(\\beta)$: distribution over topics for a given document\n",
        "\n",
        "$n_m$, $n_{mz}$, $n_{zw}$, $n_z$: as defined above"
      ]
    },
    {
      "metadata": {
        "id": "gvw2cKIYON_c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Import packages and functions"
      ]
    },
    {
      "metadata": {
        "id": "r4F2wy8yON_f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy import sqrt,mean,square\n",
        "from scipy.special import digamma, polygamma"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZXa5E7kXON_m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Word counting function"
      ]
    },
    {
      "metadata": {
        "id": "zhNlY4BoON_p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def words_count_doc(corpus):\n",
        "    \"\"\"\n",
        "    Count the total number of words in each document in corpus.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    corpus : a list-like structure, contains bag-of-words of each document\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    n_m : a np.array, shape(M)\n",
        "         the total number of words in each document\n",
        "    \"\"\"\n",
        "    n_m = []\n",
        "    for i in range(len(corpus)):\n",
        "        n_m.append(np.sum(corpus[i], axis = 0)[1])\n",
        "    return np.array(n_m)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P-XQ4CQ7ON_1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Initialize empty parameters"
      ]
    },
    {
      "metadata": {
        "id": "UqdeS7KiON_4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def empty_parameters(corpus, K, V):\n",
        "    \"\"\"\n",
        "    Initialize empty parameters n_mz, n_zw, n_z.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    K : int, the number of topics\n",
        "    V : int, the number of vocabulary\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    z_mw : the topic of word w in document m\n",
        "    n_mz : the number of words from document m assigned to topic z\n",
        "    n_zw : the number of words assigned topic z\n",
        "    n_z : the total number of words assigned to topic z\n",
        "    \"\"\"\n",
        "    z_mw = []\n",
        "    n_mz = np.zeros((len(corpus), K))\n",
        "    n_zw = np.zeros((K, V))\n",
        "    n_z = np.zeros(K)\n",
        "    return z_mw, n_mz, n_zw, n_z"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0icoaTfTOOAC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Initialize parameters based on words in documents"
      ]
    },
    {
      "metadata": {
        "id": "0qGHoG6sOOAE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def initial_parameters(corpus, K, V):\n",
        "    \"\"\"\n",
        "    Initialize parameters for the corpus \n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    corpus: a list-like structure, contains bag-of-words of each document\n",
        "    K : int, the number of topics\n",
        "    V : int, the size of the vocabulary\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    z_mw : the topic of word w in document m\n",
        "    n_mz : the number of words from document m assigned to topic z\n",
        "    n_zw : the number of words assigned topic z\n",
        "    n_z : the total number of words assigned to topic z\n",
        "    \n",
        "    \"\"\"\n",
        "    z_mw, n_mz, n_zw, n_z = empty_parameters(corpus, K, V)\n",
        "    z_mw = []\n",
        "    for m, doc in enumerate(corpus):\n",
        "        z_n = []\n",
        "        for n, t in doc:\n",
        "            z = np.random.randint(0, K)\n",
        "            z_n.append(z)\n",
        "            n_mz[m, z] += t\n",
        "            n_zw[z, n] += t\n",
        "            n_z[z] += t\n",
        "        z_mw.append(np.array(z_n))\n",
        "    return z_mw, n_mz, n_zw, n_z"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dy0FG65pOOAM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Gibbs Sampling"
      ]
    },
    {
      "metadata": {
        "id": "pIK3SwijOOAX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def gibbs_sampling(corpus, max_iter, K, V, n_zw, n_z, n_mz, n_m, z_mw, alpha, phi):\n",
        "    beta_gibbs = []\n",
        "    theta_gibbs = []\n",
        "    \n",
        "    np.random.seed(1337)\n",
        "    for i in range(max_iter):\n",
        "        if i%1000 == 0:\n",
        "            print(i)\n",
        "        for m, doc in enumerate(corpus):\n",
        "            for n, (w, t) in enumerate(doc):\n",
        "                #exclude the current word\n",
        "                z = z_mw[m][n]\n",
        "                n_mz[m, z] -= t\n",
        "                n_m[m] -= t\n",
        "                n_zw[z, w] -= t\n",
        "                n_z[z] -= t\n",
        "        \n",
        "                new_z = sample_topic(K, n_zw, n_z, n_mz, n_m, alpha, phi, w, m)\n",
        "\n",
        "                #include the current word\n",
        "                z_mw[m][n] = new_z\n",
        "                n_mz[m, new_z] += t\n",
        "                n_zw[new_z, w] += t\n",
        "                n_z[new_z] += t\n",
        "                n_m[m] += t\n",
        "\n",
        "        #update beta\n",
        "        beta_gibbs.append(update_beta(V, n_zw, n_z, alpha))\n",
        "        #update theta\n",
        "        theta_gibbs.append(update_theta(K, n_mz, n_m, phi))\n",
        "    return beta_gibbs, theta_gibbs\n",
        "\n",
        "def sample_topic(K, n_zw, n_z, n_mz, n_m, alpha, phi, w, m):\n",
        "    \"\"\"\n",
        "    Sample new topic for current word\n",
        "    \n",
        "    \"\"\"\n",
        "    p_z = np.zeros(K)\n",
        "    for j in range(K):\n",
        "        p_z[j] = ((n_zw[j, w] + phi)/(n_z[j] + V * phi)) * ((n_mz[m, j] + alpha)/(n_m[m] + K * alpha))\n",
        "    new_z = np.random.multinomial(1, p_z/p_z.sum()).argmax()\n",
        "    return new_z    \n",
        "\n",
        "def update_beta(V, n_zw, n_z, alpha):\n",
        "    \"\"\"\n",
        "    Update beta\n",
        "    \"\"\"\n",
        "    beta = (n_zw + alpha)/(n_z[:,None] + V *alpha)\n",
        "    return beta\n",
        "\n",
        "def update_theta(K, n_mz, n_m, phi):\n",
        "    \"\"\"\n",
        "    Update theta\n",
        "    \"\"\"\n",
        "    theta = (n_mz + phi)/(n_m[:, None] + K * phi)\n",
        "    return theta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1l4blvWGOOAg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.2 Implementation - Variational Inference"
      ]
    },
    {
      "metadata": {
        "id": "TfDNLWjUOOAj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Latent Dirichlet Allocation was also implemented using variational inference. In situations where variational inference is typically used, the posterior is typically intractable to calculate directly. In the case of LDA, the posterior $p(\\theta,z,w | \\alpha,\\beta)$ is difficult to compute, so the distribution is instead approximated with the variational distribution:\n",
        "\n",
        "$$q(\\theta,z | \\gamma,\\phi) = q(\\theta|\\gamma) \\prod_{n=1}^{N} q(z_n|\\phi_n)$$\n",
        "\n",
        "Using Jensen's inequality, it can be shown that the difference between the log likelihood of the true posterior and the variational approximation is the KL-divergence between the two. In order words:\n",
        "\n",
        "$$ \\log(p(w|\\alpha,\\beta) = L(\\gamma,\\phi;\\alpha,\\beta) + D(q(\\theta,z|\\gamma,\\phi)||p(\\theta,z|w,\\alpha,\\beta))$$\n",
        "\n",
        "We can choose to either minimize the KL-divergence or maximize the likelihood. Here, the latter is approach is taken. Factoring the likelihood appropriately, we can write the following:\n",
        "\n",
        "$$ L(\\gamma,\\phi;\\alpha,\\beta) = E_q[\\log p(\\theta|\\alpha)] + E_q[\\log p(z|\\theta)] + E_q [\\log p(w|z,\\beta)] - E_q [\\log q(\\theta)] - E_q[\\log q(z)] $$\n",
        "\n",
        "This likelihood is maximized through Expectation-Maximization (EM). During the expectation step, the variational parameters $\\phi$ and $\\gamma$ are first optimized by maximizing the likelihood with respect to each individually. During the maximization step, the likelihood is then maximized with respect to model parameters $\\alpha$ and $\\beta$. This process is outlined below."
      ]
    },
    {
      "metadata": {
        "id": "S14RCJb5OOAl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Variables and Parameters\n",
        "\n",
        "document:    $m = 1,...,M$\n",
        "\n",
        "topic:       $z = 1,...,k$\n",
        "\n",
        "word:        $w = 1,...,N_m$\n",
        "\n",
        "vocabulary : $v = 1,...,V$\n",
        "\n",
        "$\\alpha: 1 \\times k$ Model parameter - vector of topic distribution probabilities for each document\n",
        "\n",
        "$\\beta: k \\times v$ Model parameter - matrix of word probabilities for each topic\n",
        "\n",
        "$\\phi: M \\times N_m \\times k$ Variational parameter - matrix of topic probabilities for each word in each document\n",
        "\n",
        "$\\gamma: M \\times k$ Variational parameter - matrix of topic probabilities for each document"
      ]
    },
    {
      "metadata": {
        "id": "IdPvrAh3OOAn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Optimize variational parameters $\\phi$ and $\\gamma$\n",
        "\n",
        "By taking the derivative the log likelihood with respect to $\\phi$ and setting the result to zero, we find the maximal value of $\\phi$:\n",
        "\n",
        "$$ \\phi_{ni} \\propto \\beta_{iv} \\exp(\\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k(\\gamma_j)) $$\n",
        "\n",
        "where $\\beta_{iv}$ = $p(w_n^v = 1|z_n = i)$ and $\\Psi$ is the digamma function (derivative of the log gamma function $\\Gamma$). As $\\phi$ represents the probability of each word in a document for each latent topic, these values must be normalized such that each row representing a word position within a document must sum to 1.\n",
        "\n",
        "\n",
        "In a similar fashion, it can be shown that $\\gamma$ is maximized at:\n",
        "\n",
        "$$ \\gamma_i = \\alpha_i + \\sum_{n=1}^N(\\phi_{ni})$$"
      ]
    },
    {
      "metadata": {
        "id": "7cznN_r2OOAr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Optimize variational parameter phi\n",
        "def opt_phi(beta,gamma,words,M,N,k):\n",
        "    for m in range(M):\n",
        "        for n in range(N[m]):\n",
        "            for i in range(k):\n",
        "                phi[m][n,i] = int(np.round(beta[words[m][n],i], 0)) * int(np.round(np.exp(digamma(gamma[m,i]), 0)) - int(np.round(digamma(np.sum(gamma[m,:]))), 0))\n",
        "            # Normalize across states so phi represents probability over states for each word\n",
        "            phi[m][n,:] = phi[m][n,:]/np.sum(phi[m][n,:])\n",
        "    return phi\n",
        "\n",
        "\n",
        "## Optimize variational parameter gamma\n",
        "def opt_gamma(alpha,phi,M):\n",
        "    gamma = np.tile(alpha,(M,1)) + np.array(list(map(lambda x: np.sum(x,axis=0),phi)))\n",
        "    return gamma"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FUL6jkbMOOA7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Estimate model parameters $\\alpha$ and $\\beta$\n",
        "\n",
        "By taking the derivative of the log likelihood and applying the appropriate Lagrange multipliers to ensure probabilities sum to 1, we find that $/beta$ is maximized with:\n",
        "\n",
        "$$ \\beta_{ij} \\propto \\sum_{m=1}^M \\sum_{n=1}^{N_m} \\phi_{dni}w_{mn}^j$$\n",
        "\n",
        "where $w_{mn}^j$ = 1 if the $n^{th}$ word of document $m$ is equal to $j$, and 0 otherwise. Since the columns of \\beta represent the probability of each word given the topic of that particular column, they must be normalized to sum to 1.\n",
        "\n",
        "Taking the derivative of the log likelihood with respect to $\\alpha$ yields:\n",
        "\n",
        "$$ \\frac{\\partial L}{\\partial\\alpha_i} = M(\\Psi(\\sum_{j=1}^k\\alpha_j)-\\Psi(\\alpha_i)) - \\sum_{m=1}^M(\\Psi(\\gamma_{di})-\\Psi(\\sum_{j=1}^k\\gamma_{dj}))$$\n",
        "\n",
        "Because this is difficult to find the zero intercept of this derivative, $\\alpha$ is instead maximized numerically with the Newton-Raphson method. The Hessian is of the form:\n",
        "\n",
        "$$ \\frac{\\partial^2 L}{\\partial\\alpha_i\\partial\\alpha_j} = M(\\Psi'(\\sum_{j=1}^k \\alpha_j) - \\delta(i,j)\\Psi'(\\alpha_i))$$ \n",
        "\n",
        "Note: This is slightly different from what is stated in the paper, which has a couple errors in the reported form of the Hessian."
      ]
    },
    {
      "metadata": {
        "id": "YA-2hK2NOOA_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Optimize beta\n",
        "def est_beta(phi,words,k,V):\n",
        "    for j in range (V):\n",
        "        # Construct w_mn == j of same shape as phi\n",
        "        w_mnj = [np.tile((word == j),(k,1)).T for word in words]\n",
        "        beta[j,:] = np.sum(np.array(list(map(lambda x: np.sum(x,axis=0),phi*w_mnj))),axis=0)\n",
        "        \n",
        "    # Normalize across states so beta represents probability of each word given the state\n",
        "    for i in range(k):\n",
        "        beta[:,i] = beta[:,i]/sum(beta[:,i])\n",
        "        \n",
        "    return beta\n",
        "\n",
        "\n",
        "## Optimize alpha\n",
        "#  (Newton-Raphson method, for a Hessian with special structure)\n",
        "def est_alpha(alpha,gamma,M,k,nr_max_iters = 1000,tol = 10**-2.0):\n",
        "    for it in range(nr_max_iters):\n",
        "        alpha_old = alpha\n",
        "        \n",
        "        #  Calculate gradient \n",
        "        g = M*(digamma(np.sum(alpha))-digamma(alpha)) + np.sum(digamma(gamma)-np.tile(digamma(np.sum(gamma,axis=1)),(k,1)).T,axis=0)\n",
        "        #  Calculate Hessian diagonal component\n",
        "        h = -M*polygamma(1,alpha) \n",
        "        #  Calculate Hessian constant component\n",
        "        z = M*polygamma(1,np.sum(alpha))\n",
        "        #  Calculate constant\n",
        "        c = np.sum(g/h)/(z**(-1.0)+np.sum(h**(-1.0)))\n",
        "\n",
        "        #  Update alpha\n",
        "        alpha = alpha - (g-c)/h\n",
        "        \n",
        "        #  Check convergence\n",
        "        if sqrt(mean(square(alpha-alpha_old)))<tol:\n",
        "            break\n",
        "        \n",
        "    return alpha"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3lHItxksOODS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4. Conclusion"
      ]
    },
    {
      "metadata": {
        "id": "LC9avpSEOODT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We implemented in Python via Gibbs Sampling and Variational Inference. Using the Dirchlet distribution as a prior, the goal was to discover latent topics among documents in a corpus. We simulated a dataset according to the generative process in Blei et al's paper. We then used LDA to estimate the latent variables and compared them with real latent variables in the generating process. We can try to improve the performance of our Gibbs sampling method with Cython."
      ]
    },
    {
      "metadata": {
        "id": "exVzH8OtOODU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## References"
      ]
    },
    {
      "metadata": {
        "id": "2hm9PT13OODV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "[1] Blei, David M., Andrew Y. Ng, and Michael I. Jordan. \"Latent dirichlet allocation.\" the Journal of machine Learning research 3 (2003): 993-1022.\n",
        "\n",
        "[2] Griffiths, Tom. \"Gibbs sampling in the generative model of latent dirichlet allocation.\" (2002)."
      ]
    }
  ]
}